{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3aa4948462094761a07b1a8fa3ff302c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7709b6ee44940569d57231cc3b82a01",
              "IPY_MODEL_a5eec28f65b34749b1bac14ede05616d",
              "IPY_MODEL_af9a9a2d41e64cf8a9dae384b8e38eb7"
            ],
            "layout": "IPY_MODEL_75c8434337744b52b8d8c461bc3d063d"
          }
        },
        "b7709b6ee44940569d57231cc3b82a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c458935d18bd409ba4bc2602f6490ed6",
            "placeholder": "​",
            "style": "IPY_MODEL_0f3b4b429f654e519337dc30ef69ec29",
            "value": ""
          }
        },
        "a5eec28f65b34749b1bac14ede05616d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05590b3be52e4339bb20a5f413f12a13",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a59a7795e8a9406ea31f21cf17120a3f",
            "value": 0
          }
        },
        "af9a9a2d41e64cf8a9dae384b8e38eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2be65178a765496c9de99332a89d10f2",
            "placeholder": "​",
            "style": "IPY_MODEL_8d138663ac074f32b97d7ed6c8db5dc5",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "75c8434337744b52b8d8c461bc3d063d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c458935d18bd409ba4bc2602f6490ed6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f3b4b429f654e519337dc30ef69ec29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05590b3be52e4339bb20a5f413f12a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a59a7795e8a9406ea31f21cf17120a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2be65178a765496c9de99332a89d10f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d138663ac074f32b97d7ed6c8db5dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e4f574581a5423ab9e66a9b57c5feed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86665b50777040e099f59c371c83a97c",
              "IPY_MODEL_f6ab28713988425992052c733dcd5845",
              "IPY_MODEL_dd4c7e82118a4a5190b35bbd603523a1"
            ],
            "layout": "IPY_MODEL_567c8224286c4866914af3b40eca9c49"
          }
        },
        "86665b50777040e099f59c371c83a97c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e509ae39fd1a41b59a804fe479acbcb9",
            "placeholder": "​",
            "style": "IPY_MODEL_472e18d5a7f54ccbb105d32e580ea73e",
            "value": "Batches: 100%"
          }
        },
        "f6ab28713988425992052c733dcd5845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db42543b819048c8a7afb3bed057aeb8",
            "max": 44,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36638dc1b8fe4c5b8c07b9dfab1806c9",
            "value": 44
          }
        },
        "dd4c7e82118a4a5190b35bbd603523a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e660bc0eec6f4d4ba41da66d63d32127",
            "placeholder": "​",
            "style": "IPY_MODEL_2e3272cea6ed4a1491d6191126ef7f68",
            "value": " 44/44 [01:45&lt;00:00,  2.52s/it]"
          }
        },
        "567c8224286c4866914af3b40eca9c49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e509ae39fd1a41b59a804fe479acbcb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "472e18d5a7f54ccbb105d32e580ea73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db42543b819048c8a7afb3bed057aeb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36638dc1b8fe4c5b8c07b9dfab1806c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e660bc0eec6f4d4ba41da66d63d32127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e3272cea6ed4a1491d6191126ef7f68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d529f20a1fed48fd932ff525aa27666e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_982368cc0afb445e97f3656bf5d82f59",
              "IPY_MODEL_fed18b8518b343088017cece37de8106",
              "IPY_MODEL_8c70555a9d3141a5b04bdd008a562859"
            ],
            "layout": "IPY_MODEL_67067197449a4d82875ce44d31631bc3"
          }
        },
        "982368cc0afb445e97f3656bf5d82f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4068b0f14f1642d1805be13ebef55669",
            "placeholder": "​",
            "style": "IPY_MODEL_40825c5726c44974bf347fb756e1e162",
            "value": "Batches: 100%"
          }
        },
        "fed18b8518b343088017cece37de8106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a190f079c214f7b9e2a662085279a4f",
            "max": 44,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac4cf65358d84e358cad39313d15bebc",
            "value": 44
          }
        },
        "8c70555a9d3141a5b04bdd008a562859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7889e3e425e74209a4f8912c3eb14736",
            "placeholder": "​",
            "style": "IPY_MODEL_3b62ef3c9dea4fbd8c0042dd0a68c327",
            "value": " 44/44 [01:54&lt;00:00,  2.50s/it]"
          }
        },
        "67067197449a4d82875ce44d31631bc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4068b0f14f1642d1805be13ebef55669": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40825c5726c44974bf347fb756e1e162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a190f079c214f7b9e2a662085279a4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac4cf65358d84e358cad39313d15bebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7889e3e425e74209a4f8912c3eb14736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b62ef3c9dea4fbd8c0042dd0a68c327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augmented Generation\n",
        "\n",
        "By Peter Nadel, Digital Humanities Natural Language Processing Specialist\n",
        "\n",
        "\n",
        "## Background\n",
        "\n",
        "Retrieval Augmented Generation, or RAG, is a technique that can be used to prompt a Large Language Model (LLM) to answer questions from an existing knowledge base. It doesn't not involve any further training of the model, but rather relies on the model's ability to follow instructions.\n",
        "\n",
        "## Two-Step Process\n",
        "Given a user query, RAG decomposes into two main steps. First, we have to go into the knowledge base, what practitioners sometimes call a *corpus*, and retrieve sections of the text that are relevant to the user query. This process is usually known as information retrieval and has been a common task since the inception of NLP. We will implement an algorithm called 'semantic search' to retrieve these text chunks from our corpus. Second, we will pass these retrieved chunks to the LLM in the form of an elaborate prompt, telling the LLM to answer the user query only refering to the information in the context.      \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lJ8LX2XZPJRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvOn873cPE8Q"
      },
      "outputs": [],
      "source": [
        "# install dependancies\n",
        "%%capture\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir -q\n",
        "else:\n",
        "  !pip install llama-cpp-python -U\n",
        "\n",
        "!pip install sentence_transformers --no-deps -q\n",
        "!pip install streamlit langchain pypdf python-docx pandas numpy tiktoken huggingface-hub -q\n",
        "!pip install numpy==1.23.5 -q\n",
        "!mkdir BAAI_bge-m3\n",
        "!huggingface-cli download BAAI/bge-m3 --local-dir BAAI_bge-m3 --local-dir-use-symlinks False\n",
        "!chmod -R 755 BAAI_bge-m3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependancies\n",
        "import streamlit as st\n",
        "import os\n",
        "from llama_cpp import Llama\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pypdf\n",
        "import docx\n",
        "from io import StringIO, BytesIO\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "import re\n",
        "import requests\n",
        "import io\n",
        "from IPython.display import display, HTML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "3aa4948462094761a07b1a8fa3ff302c",
            "b7709b6ee44940569d57231cc3b82a01",
            "a5eec28f65b34749b1bac14ede05616d",
            "af9a9a2d41e64cf8a9dae384b8e38eb7",
            "75c8434337744b52b8d8c461bc3d063d",
            "c458935d18bd409ba4bc2602f6490ed6",
            "0f3b4b429f654e519337dc30ef69ec29",
            "05590b3be52e4339bb20a5f413f12a13",
            "a59a7795e8a9406ea31f21cf17120a3f",
            "2be65178a765496c9de99332a89d10f2",
            "8d138663ac074f32b97d7ed6c8db5dc5"
          ]
        },
        "id": "5gs6HUtjllBH",
        "outputId": "adef6922-1176-44b0-a7a0-cc7e94c90fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3aa4948462094761a07b1a8fa3ff302c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Semantic Search\n",
        "\n",
        "Semantic search utilizes a different language model to represent text chunks in high dimensional vector space, often called an *embedding*. We will split up our corpus into chunks of text so that then we can *embed* it using this model. Once embedded, we can then embed our user query in the same way.\n",
        "\n",
        "We will then have one matrix of shape (number_of_text_chunks, embedding_dimension) and a one vector (1, embedding_dimension). Taking the dot product between this vector and the transpose of this matrix will give us a (1, number_of_text_chunks) vector the values of which represent the **similarity between the query and each text chunk**.\n",
        "\n",
        "In standard semantic search, we would then return the results back to the user, but in the case of RAG, we will use these text chunks as the context in a prompt.\n",
        "\n",
        "**Nota Bene**: The term \"large language model\" was originally coined to describe these embedding models. In fact, they are remarkably similar to their \"generative\" counter parts. The main difference is that an embedding model is trained guess randomly masked words from a larger sequence, whereas what is commonly refered to as an LLM is trained to guess the next word in a large sequence. The former is very good for modeling semantic meaning in texts, and the latter is very good for text completion."
      ],
      "metadata": {
        "id": "r8aWQmljPQ7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data prep\n",
        "\n",
        "In this example, we'll be looking at the first volume of *The Decline and Fall of the Roman Empire* by Edward Gibbon. This is massive text about the the Roman Empire from 200 AD to the Fall of Constninople in 1453. This selection covers the migration and integration of Germanic tribes into the remants of the Western Roman Empire around 400 AD.\n",
        "\n",
        "It is a useful case to explore with RAG as it is very long and thus cannot be given in its totality to an LLM. Instead, we will have to employ RAG so that the LLM answers accurately and quickly.\n",
        "\n",
        "Here we will prepare our data for RAG."
      ],
      "metadata": {
        "id": "wh6PL637P35g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the url to the gutenberg project page\n",
        "display(HTML(\n",
        "    \"\"\"<iframe src=\"https://gutenberg.org/cache/epub/731/pg731.txt\"></iframe>\"\"\"\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "bf0LOPe0ZwTw",
        "outputId": "fb42145f-1d8c-40b2-acca-4c1ec0207556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/display.py:724: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe src=\"https://gutenberg.org/cache/epub/731/pg731.txt\"></iframe>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.get('https://gutenberg.org/cache/epub/731/pg731.txt') # using requests to get the text\n",
        "text = res.text\n",
        "text[10000:10500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "dpV6YJyOAT6T",
        "outputId": "f6a5f7e8-b351-4332-e0a7-7fe5e7f4c914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' has cast the decay and ruin of the ancient civilization, the\\r\\n      formation and birth of the new order of things, will of itself,\\r\\n      independent of the laborious execution of his immense plan,\\r\\n      render “The Decline and Fall of the Roman Empire” an\\r\\n      unapproachable subject to the future historian: 101 in the\\r\\n      eloquent language of his recent French editor, M. Guizot:—\\r\\n\\r\\n      101 (return) [ A considerable portion of this preface has already\\r\\n      appeared before us public '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking\n",
        "\n",
        "Now that we have our text, we need to embed it. Before we can do so, we need to split it up in to chunks that can be read by the embedding model. This process, known as chunking, can have a profound effect on the output of our RAG. If our chunks are too small or too big then our context will be useless, so it often comes down to experiementation.\n",
        "\n",
        "We will use an off-the-shelf chunker from `langchain` for this example, but I encourage your to design your own. I've given a default `chunk_size` of 250 tokens and `chunk_overlap`, how much from the last chunk should be carried over into the current chunk, of 50 tokens. Play around with this and see the differences."
      ],
      "metadata": {
        "id": "lY57oyOtaJH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = TokenTextSplitter(chunk_size=250, chunk_overlap=50) # langchain tokentextsplitter, other exists: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\n",
        "chunks = text_splitter.split_text(text)\n",
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAd5nGQwzK_6",
        "outputId": "b6f1e425-1740-480d-913f-499079dd91a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2807"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding model: `bge-m3`\n",
        "\n",
        "Below we begin the embedding process for our corpus. We are using this embedding model: [bge-m3](https://huggingface.co/BAAI/bge-m3). I like this one from experimentation but there are many other that you can find on HuggingFace. I encourage you to experiment with this choice as well."
      ],
      "metadata": {
        "id": "TIch2DaxbVKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # move data to the gpu\n",
        "model = SentenceTransformer('/content/BAAI_bge-m3')\n",
        "\n",
        "embeddings = model.encode(\n",
        "    chunks, # text input\n",
        "    batch_size=64, # batch size\n",
        "    device=device, # gpu device\n",
        "    show_progress_bar=True,\n",
        "    convert_to_tensor=True, # converts to Pytorch tensor\n",
        "    normalize_embeddings=True # allows us to compare embeddings\n",
        ")\n",
        "\n",
        "embeddings.shape # (number_of_text_chunks, embedding_dimension)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "8e4f574581a5423ab9e66a9b57c5feed",
            "86665b50777040e099f59c371c83a97c",
            "f6ab28713988425992052c733dcd5845",
            "dd4c7e82118a4a5190b35bbd603523a1",
            "567c8224286c4866914af3b40eca9c49",
            "e509ae39fd1a41b59a804fe479acbcb9",
            "472e18d5a7f54ccbb105d32e580ea73e",
            "db42543b819048c8a7afb3bed057aeb8",
            "36638dc1b8fe4c5b8c07b9dfab1806c9",
            "e660bc0eec6f4d4ba41da66d63d32127",
            "2e3272cea6ed4a1491d6191126ef7f68"
          ]
        },
        "id": "QLWFChDsmRT0",
        "outputId": "259acc04-2866-4cf0-c0c8-33e7e9e708ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e4f574581a5423ab9e66a9b57c5feed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2807, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information retrieval\n",
        "\n",
        "Now we are ready to start the information retrieval process. As mentioned above, we will take a question from the user, here `query` and search for similar text chunks with a matrix multiplication. We will then extract the indices of the most relevant chunks and compile them into a form to be read by our LLM."
      ],
      "metadata": {
        "id": "muHMKNRusdvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_instruction = \"Represent this sentence for searching relevant passages: \" # need to prepend this string\n",
        "query = \"What were the major goals of the Antonines?\" # user query\n",
        "query_embedding = model.encode(retrieval_instruction+query, device=device, convert_to_tensor=True, normalize_embeddings=True)\n",
        "sim_vector = (embeddings.to(device) @ query_embedding.to(device)) # matmul to compare query_embedding to embeddings\n",
        "sim_vector.shape # simiarlity scores between each embedding and the query_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHh3L2xynSMq",
        "outputId": "068df189-d59f-48f1-d9c1-d3da37c506c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2807])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim_vector.argsort() # sorted by index, same index as chunk_list!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBdZ8MHgoLDX",
        "outputId": "f904cd26-998d-449d-ab5f-ec74307793fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2794, 2798, 2795,  ...,  131,  438,  430], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim_vector.argsort().cpu().numpy() # takes off of gpu and converts to a numpy array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVIAttWAoJex",
        "outputId": "83d883f2-e87f-418a-ccd2-65b029b201d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2794, 2798, 2795, ...,  131,  438,  430])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim_vector.argsort().cpu().numpy()[::-1] # reverses list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8h8brSvoutN",
        "outputId": "eaa6c96a-58ed-4331-aa8d-13f75a6961e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 430,  438,  131, ..., 2795, 2798, 2794])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_indices = sim_vector.argsort().cpu().numpy()[::-1][:10] # get top 10, this number is arbitrary\n",
        "top_10_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjJJlOhjox3L",
        "outputId": "5aeea6c4-2017-4435-940f-c4c59701131f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([430, 438, 131, 585,   3,   4, 261,  93, 189,  94])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_chunks = [chunks[i] for i in top_10_indices]\n",
        "top_10_chunks # most relevant chunks to our search query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJMehE29o3_s",
        "outputId": "dc5e0476-dcef-4efa-e6cc-eca62d766ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['   Antonines, who were themselves men of learning and curiosity. It\\r\\n      was diffused over the whole extent of their empire; the most\\r\\n      northern tribes of Britons had acquired a taste for rhetoric;\\r\\n      Homer as well as Virgil were transcribed and studied on the banks\\r\\n      of the Rhine and Danube; and the most liberal rewards sought out\\r\\n      the faintest glimmerings of literary merit. 110 The sciences of\\r\\n      physic and astronomy were successfully cultivated by the Greeks;\\r\\n      the observations of Ptolemy and the writings of Galen are studied\\r\\n      by those who have improved their discoveries and corrected their\\r\\n      errors; but if we except the inimitable Lucian, this age of\\r\\n      indolence passed away without having produced a single writer of\\r\\n      original genius, or who excelled in the arts of elegant\\r\\n      composition.1101 The authority of Plato and',\n",
              " ' the fierce giants of the north broke in, and mended\\r\\n      the puny breed. They restored a manly spirit of freedom; and\\r\\n      after the revolution of ten centuries, freedom became the happy\\r\\n      parent of taste and science.\\r\\n\\r\\n      111 (return) [ Longin. de Sublim. c. 44, p. 229, edit. Toll.\\r\\n      Here, too, we may say of Longinus, “his own example strengthens\\r\\n      all his laws.” Instead of proposing his sentiments with a manly\\r\\n      boldness, he insinuates them with the most guarded caution; puts\\r\\n      them into the mouth of a friend, and as far as we can collect\\r\\n      from a corrupted text, makes a show of refuting them himself.]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      Chapter III: The Constitution In The Age Of The Antonines.—Part\\r\\n      I.\\r\\n\\r\\n',\n",
              " '      Note: The journeys of Hadrian are traced in a note on Solvet’s\\r\\n      translation of Hegewisch, Essai sur l’Epoque de Histoire Romaine\\r\\n      la plus heureuse pour Genre Humain Paris, 1834, p. 123.—M.]\\r\\n\\r\\n      26 (return) [ See the Augustan History and the Epitomes.]\\r\\n\\r\\n      Notwithstanding this difference in their personal conduct, the\\r\\n      general system of Augustus was equally adopted and uniformly\\r\\n      pursued by Hadrian and by the two Antonines. They persisted in\\r\\n      the design of maintaining the dignity of the empire, without\\r\\n      attempting to enlarge its limits. By every honorable expedient\\r\\n      they invited the friendship of the barbarians; and endeavored to\\r\\n      convince mankind that the Roman power, raised above the\\r\\n      temptation of conquest, was actuated only by the love of',\n",
              " ' In the enjoyment of a great estate, they never\\r\\n      admitted the idea of a separate interest: some fragments are now\\r\\n      extant of a treatise which they composed in common; 152 and in\\r\\n      every action of life it was observed that their two bodies were\\r\\n      animated by one soul. The Antonines, who valued their virtues,\\r\\n      and delighted in their union, raised them, in the same year, to\\r\\n      the consulship; and Marcus afterwards intrusted to their joint\\r\\n      care the civil administration of Greece, and a great military\\r\\n      command, in which they obtained a signal victory over the\\r\\n      Germans. The kind cruelty of Commodus united them in death. 16\\r\\n\\r\\n      152 (return) [ This work was on agriculture, and is often quoted\\r\\n      by later writers. See P. Needham, Proleg. ad Geoponic. Camb.\\r\\n      1704.—',\n",
              " '\\r\\n         Chapter I: The Extent Of The Empire In The Age Of The\\r\\n         Antonines.—Part III.\\r\\n\\r\\n         Chapter II: The Internal Prosperity In The Age Of The\\r\\n         Antonines.—Part I.\\r\\n\\r\\n     Of The Union And Internal Prosperity Of The Roman Empire, In The\\r\\n     Age Of The Antonines.\\r\\n\\r\\n         Chapter II: The Internal Prosperity In The Age Of The\\r\\n         Antonines.—Part II.\\r\\n\\r\\n         Chapter II: The Internal Prosperity In The Age Of The\\r\\n         Antonines.—Part III.\\r\\n\\r\\n         Chapter II: The Internal Prosperity In The Age Of The\\r\\n         Antonines. Part IV.\\r\\n\\r\\n         Chapter III: The',\n",
              " '       Chapter II: The Internal Prosperity In The Age Of The\\r\\n         Antonines. Part IV.\\r\\n\\r\\n         Chapter III: The Constitution In The Age Of The\\r\\n         Antonines.—Part I.\\r\\n\\r\\n     Of The Constitution Of The Roman Empire, In The Age Of The\\r\\n     Antonines.\\r\\n\\r\\n         Chapter III: The Constitution In The Age Of The\\r\\n         Antonines.—Part II.\\r\\n\\r\\n         Chapter IV: The Cruelty, Follies And Murder Of Commodus.—Part\\r\\n         I.\\r\\n\\r\\n     The Cruelty, Follies, And Murder Of Commodus—Election Of\\r\\n     Pertinax—His Attempts To Reform The State—His Assassination By The\\r\\n     Prætorian Guards.\\r\\n\\r\\n    ',\n",
              " ' us through the dark and profound\\r\\n      abyss. He represents with candor, and confutes with subtlety, the\\r\\n      opinions of the philosophers.]\\r\\n\\r\\n      7 (return) [ I do not pretend to assert, that, in this\\r\\n      irreligious age, the natural terrors of superstition, dreams,\\r\\n      omens, apparitions, &c., had lost their efficacy.]\\r\\n\\r\\n      Notwithstanding the fashionable irreligion which prevailed in the\\r\\n      age of the Antonines, both the interest of the priests and the\\r\\n      credulity of the people were sufficiently respected. In their\\r\\n      writings and conversation, the philosophers of antiquity asserted\\r\\n      the independent dignity of reason; but they resigned their\\r\\n      actions to the commands of law and of custom. Viewing, with a\\r\\n      smile of pity and indulgence, the various errors of the vulgar,\\r\\n     ',\n",
              " ' The Extent Of The Empire In The Age Of The\\r\\n      Antonines—Part I.\\r\\n\\r\\n      Introduction.\\r\\n\\r\\n     The Extent And Military Force Of The Empire In The Age Of The\\r\\n     Antonines.\\r\\n\\r\\n      In the second century of the Christian Æra, the empire of Rome\\r\\n      comprehended the fairest part of the earth, and the most\\r\\n      civilized portion of mankind. The frontiers of that extensive\\r\\n      monarchy were guarded by ancient renown and disciplined valor.\\r\\n      The gentle but powerful influence of laws and manners had\\r\\n      gradually cemented the union of the provinces. Their peaceful\\r\\n      inhabitants enjoyed and abused the advantages of wealth and\\r\\n      luxury. The image of a free constitution was preserved with\\r\\n      decent reverence: the Roman senate appeared to possess the\\r\\n      sovereign authority, and devolved on the emperors all',\n",
              " '    extraordinary effort.]\\r\\n\\r\\n      We have attempted to explain the spirit which moderated, and the\\r\\n      strength which supported, the power of Hadrian and the Antonines.\\r\\n      We shall now endeavor, with clearness and precision, to describe\\r\\n      the provinces once united under their sway, but, at present,\\r\\n      divided into so many independent and hostile states. Spain, the\\r\\n      western extremity of the empire, of Europe, and of the ancient\\r\\n      world, has, in every age, invariably preserved the same natural\\r\\n      limits; the Pyrenæan Mountains, the Mediterranean, and the\\r\\n      Atlantic Ocean. That great peninsula, at present so unequally\\r\\n      divided between two sovereigns, was distributed by Augustus into\\r\\n      three provinces, Lusitania, Bætica, and Tarraconensis. The\\r\\n      kingdom of Portugal now fills the place of the',\n",
              " '     luxury. The image of a free constitution was preserved with\\r\\n      decent reverence: the Roman senate appeared to possess the\\r\\n      sovereign authority, and devolved on the emperors all the\\r\\n      executive powers of government. During a happy period of more\\r\\n      than fourscore years, the public administration was conducted by\\r\\n      the virtue and abilities of Nerva, Trajan, Hadrian, and the two\\r\\n      Antonines. It is the design of this, and of the two succeeding\\r\\n      chapters, to describe the prosperous condition of their empire;\\r\\n      and afterwards, from the death of Marcus Antoninus, to deduce the\\r\\n      most important circumstances of its decline and fall; a\\r\\n      revolution which will ever be remembered, and is still felt by\\r\\n      the nations of the earth.\\r\\n\\r\\n      The principal conquests of the Romans were achieved under the\\r\\n      republic']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Generation\n",
        "\n",
        "With our relevant text chunks, we can now move on to generating an answer to our user query. Prompt engineering can be deceptively difficult. I've provided a very simple prompt below, but feel free to change it and see how that affects the output of the model.\n",
        "\n",
        "For this example, we are using `llama-cpp-python` to load the pre-quantized version of [this LLM](https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B). This is finetuned version of the base Llama-3-8B model and is particularly good a following user instructions."
      ],
      "metadata": {
        "id": "kqx63ndElYs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what can we do with these chunks?\n",
        "base_prompt = \"\"\"\n",
        "# Assistant Task\n",
        "Please answer the user query only with reference to the context passages below.\n",
        "\n",
        "## Context\n",
        "{chunks}\n",
        "\n",
        "## User Query\n",
        "{query}\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "T653kRDGRLZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filled_prompt = base_prompt.format(chunks='\\n'.join(top_10_chunks), query=query) # filling our prompt with our text chunks\n",
        "filled_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "8jFZaqYzp6Ll",
        "outputId": "08b72d43-25bd-4a16-ef87-6e4fab2fa081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Assistant Task\\nPlease answer the user query only with reference to the context passages below.\\n\\n## Context\\n   Antonines, who were themselves men of learning and curiosity. It\\r\\n      was diffused over the whole extent of their empire; the most\\r\\n      northern tribes of Britons had acquired a taste for rhetoric;\\r\\n      Homer as well as Virgil were transcribed and studied on the banks\\r\\n      of the Rhine and Danube; and the most liberal rewards sought out\\r\\n      the faintest glimmerings of literary merit. 110 The sciences of\\r\\n      physic and astronomy were successfully cultivated by the Greeks;\\r\\n      the observations of Ptolemy and the writings of Galen are studied\\r\\n      by those who have improved their discoveries and corrected their\\r\\n      errors; but if we except the inimitable Lucian, this age of\\r\\n      indolence passed away without having produced a single writer of\\r\\n      original genius, or who excelled in the arts of elegant\\r\\n      composition.1101 The authority of Plato and\\n the fierce giants of the north broke in, and mended\\r\\n      the puny breed. They restored a manly spirit of freedom; and\\r\\n      after the revolution of ten centuries, freedom became the happy\\r\\n      parent of taste and science.\\r\\n\\r\\n      111 (return) [ Longin. de Sublim. c. 44, p. 229, edit. Toll.\\r\\n      Here, too, we may say of Longinus, “his own example strengthens\\r\\n      all his laws.” Instead of proposing his sentiments with a manly\\r\\n      boldness, he insinuates them with the most guarded caution; puts\\r\\n      them into the mouth of a friend, and as far as we can collect\\r\\n      from a corrupted text, makes a show of refuting them himself.]\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n      Chapter III: The Constitution In The Age Of The Antonines.—Part\\r\\n      I.\\r\\n\\r\\n\\n      Note: The journeys of Hadrian are traced in a note on Solvet’s\\r\\n      translation of Hegewisch, Essai sur l’Epoque de Histoire Romaine\\r\\n      la plus heureuse pour Genre Humain Paris, 1834, p. 123.—M.]\\r\\n\\r\\n      26 (return) [ See the Augustan History and the Epitomes.]\\r\\n\\r\\n      Notwithstanding this difference in their personal conduct, the\\r\\n      general system of Augustus was equally adopted and uniformly\\r\\n      pursued by Hadrian and by the two Antonines. They persisted in\\r\\n      the design of maintaining the dignity of the empire, without\\r\\n      attempting to enlarge its limits. By every honorable expedient\\r\\n      they invited the friendship of the barbarians; and endeavored to\\r\\n      convince mankind that the Roman power, raised above the\\r\\n      temptation of conquest, was actuated only by the love of\\n In the enjoyment of a great estate, they never\\r\\n      admitted the idea of a separate interest: some fragments are now\\r\\n      extant of a treatise which they composed in common; 152 and in\\r\\n      every action of life it was observed that their two bodies were\\r\\n      animated by one soul. The Antonines, who valued their virtues,\\r\\n      and delighted in their union, raised them, in the same year, to\\r\\n      the consulship; and Marcus afterwards intrusted to their joint\\r\\n      care the civil administration of Greece, and a great military\\r\\n      command, in which they obtained a signal victory over the\\r\\n      Germans. The kind cruelty of Commodus united them in death. 16\\r\\n\\r\\n      152 (return) [ This work was on agriculture, and is often quoted\\r\\n      by later writers. See P. Needham, Proleg. ad Geoponic. Camb.\\r\\n      1704.—\\n\\r\\n         Chapter I: The Extent Of The Empire In The Age Of The\\r\\n         Antonines.—Part III.\\r\\n\\r\\n         Chapter II: The Internal Prosperity In The Age Of The\\r\\n         Antonines.—Part I.\\r\\n\\r\\n     Of The Union And Internal Prosperity Of The Roman Empire, In The\\r\\n     Age Of The Antonines.\\r\\n\\r\\n         Chapter II: The Internal Prosperity In The Age Of The\\r\\n         Antonines.—Part II.\\r\\n\\r\\n         Chapter II: The Internal Prosperity In The Age Of The\\r\\n         Antonines.—Part III.\\r\\n\\r\\n         Chapter II: The Internal Prosperity In The Age Of The\\r\\n         Antonines. Part IV.\\r\\n\\r\\n         Chapter III: The\\n       Chapter II: The Internal Prosperity In The Age Of The\\r\\n         Antonines. Part IV.\\r\\n\\r\\n         Chapter III: The Constitution In The Age Of The\\r\\n         Antonines.—Part I.\\r\\n\\r\\n     Of The Constitution Of The Roman Empire, In The Age Of The\\r\\n     Antonines.\\r\\n\\r\\n         Chapter III: The Constitution In The Age Of The\\r\\n         Antonines.—Part II.\\r\\n\\r\\n         Chapter IV: The Cruelty, Follies And Murder Of Commodus.—Part\\r\\n         I.\\r\\n\\r\\n     The Cruelty, Follies, And Murder Of Commodus—Election Of\\r\\n     Pertinax—His Attempts To Reform The State—His Assassination By The\\r\\n     Prætorian Guards.\\r\\n\\r\\n    \\n us through the dark and profound\\r\\n      abyss. He represents with candor, and confutes with subtlety, the\\r\\n      opinions of the philosophers.]\\r\\n\\r\\n      7 (return) [ I do not pretend to assert, that, in this\\r\\n      irreligious age, the natural terrors of superstition, dreams,\\r\\n      omens, apparitions, &c., had lost their efficacy.]\\r\\n\\r\\n      Notwithstanding the fashionable irreligion which prevailed in the\\r\\n      age of the Antonines, both the interest of the priests and the\\r\\n      credulity of the people were sufficiently respected. In their\\r\\n      writings and conversation, the philosophers of antiquity asserted\\r\\n      the independent dignity of reason; but they resigned their\\r\\n      actions to the commands of law and of custom. Viewing, with a\\r\\n      smile of pity and indulgence, the various errors of the vulgar,\\r\\n     \\n The Extent Of The Empire In The Age Of The\\r\\n      Antonines—Part I.\\r\\n\\r\\n      Introduction.\\r\\n\\r\\n     The Extent And Military Force Of The Empire In The Age Of The\\r\\n     Antonines.\\r\\n\\r\\n      In the second century of the Christian Æra, the empire of Rome\\r\\n      comprehended the fairest part of the earth, and the most\\r\\n      civilized portion of mankind. The frontiers of that extensive\\r\\n      monarchy were guarded by ancient renown and disciplined valor.\\r\\n      The gentle but powerful influence of laws and manners had\\r\\n      gradually cemented the union of the provinces. Their peaceful\\r\\n      inhabitants enjoyed and abused the advantages of wealth and\\r\\n      luxury. The image of a free constitution was preserved with\\r\\n      decent reverence: the Roman senate appeared to possess the\\r\\n      sovereign authority, and devolved on the emperors all\\n    extraordinary effort.]\\r\\n\\r\\n      We have attempted to explain the spirit which moderated, and the\\r\\n      strength which supported, the power of Hadrian and the Antonines.\\r\\n      We shall now endeavor, with clearness and precision, to describe\\r\\n      the provinces once united under their sway, but, at present,\\r\\n      divided into so many independent and hostile states. Spain, the\\r\\n      western extremity of the empire, of Europe, and of the ancient\\r\\n      world, has, in every age, invariably preserved the same natural\\r\\n      limits; the Pyrenæan Mountains, the Mediterranean, and the\\r\\n      Atlantic Ocean. That great peninsula, at present so unequally\\r\\n      divided between two sovereigns, was distributed by Augustus into\\r\\n      three provinces, Lusitania, Bætica, and Tarraconensis. The\\r\\n      kingdom of Portugal now fills the place of the\\n     luxury. The image of a free constitution was preserved with\\r\\n      decent reverence: the Roman senate appeared to possess the\\r\\n      sovereign authority, and devolved on the emperors all the\\r\\n      executive powers of government. During a happy period of more\\r\\n      than fourscore years, the public administration was conducted by\\r\\n      the virtue and abilities of Nerva, Trajan, Hadrian, and the two\\r\\n      Antonines. It is the design of this, and of the two succeeding\\r\\n      chapters, to describe the prosperous condition of their empire;\\r\\n      and afterwards, from the death of Marcus Antoninus, to deduce the\\r\\n      most important circumstances of its decline and fall; a\\r\\n      revolution which will ever be remembered, and is still felt by\\r\\n      the nations of the earth.\\r\\n\\r\\n      The principal conquests of the Romans were achieved under the\\r\\n      republic\\n\\n## User Query\\nWhat were the major goals of the Antonines?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model onto the GPU\n",
        "llm = Llama.from_pretrained(\n",
        "        repo_id=\"NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF\",\n",
        "        filename=\"*Q4_K_M.gguf\",\n",
        "        verbose=False,\n",
        "        n_gpu=-1,\n",
        "        n_ctx=5000\n",
        "    )"
      ],
      "metadata": {
        "id": "IAA6-S0pqHAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a conversation\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant who answers questions.\"}, # system level prompt, feel free to experiement with this too\n",
        "    {\"role\":\"user\", \"content\":filled_prompt}\n",
        "]"
      ],
      "metadata": {
        "id": "RBMSLrbpq3RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# streamed response\n",
        "max_width = 70\n",
        "current_length = 0\n",
        "\n",
        "text = ''\n",
        "for token in llm.create_chat_completion(messages, max_tokens=-1, stream=True):\n",
        "    if 'content' in token['choices'][0]['delta']:\n",
        "        if current_length + len(token['choices'][0]['delta']['content']) + 1 > max_width:\n",
        "            print()\n",
        "            current_length = 0\n",
        "        text += token['choices'][0]['delta']['content']\n",
        "        print(token['choices'][0]['delta']['content'], end='', flush=True)\n",
        "        current_length += len(token['choices'][0]['delta']['content']) + 1\n",
        "print()\n",
        "messages.append({\"role\": \"assistant\", \"content\": text})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzCW99E1r6aY",
        "outputId": "1f3e3f5e-09dd-4038-8ae7-db0b39c72919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The major goals of the Antonines, according to the context\n",
            " passage provided, were:\n",
            "\n",
            "1. To maintain the dignity and\n",
            " stability of the Roman Empire without attempting to enlarge\n",
            " its limits.\n",
            "2. To invite the friendship of the barbar\n",
            "ians through honorable means.\n",
            "3. To cultivate the sciences\n",
            ", arts, and literature by promoting learning and rewarding\n",
            " merit.\n",
            "4. To restore a manly spirit of freedom and\n",
            " encourage original genius in writing.\n",
            "\n",
            "The Antonines,\n",
            " specifically Hadrian and Marcus Aurelius, worked together\n",
            " to achieve these goals and were known for their virtuous\n",
            " conduct and administration of the empire. They also shared\n",
            " a joint interest in various fields such as agriculture,\n",
            " military affairs, and civil administration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full process\n",
        "\n",
        "Below I will refactor the code above into a couple functions for your ease of use later on."
      ],
      "metadata": {
        "id": "AVGfRv5Xe5Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sem search functions\n",
        "def get_text_chunks(chunk_size=250, chunk_overlap=50):\n",
        "    res = requests.get('https://gutenberg.org/cache/epub/731/pg731.txt')\n",
        "    text = res.text\n",
        "\n",
        "    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "def get_embedding(chunks):\n",
        "    model = SentenceTransformer('/content/BAAI_bge-m3')\n",
        "    embeddings = model.encode(\n",
        "        chunks,\n",
        "        batch_size=64,\n",
        "        device='cuda',\n",
        "        show_progress_bar=True,\n",
        "        convert_to_tensor=True,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "def semantic_search(query, embeddings, chunks, model, device='cuda', k=10):\n",
        "    retrieval_instruction = \"Represent this sentence for searching relevant passages: \" # need to prepend this string\n",
        "    query_embedding = model.encode(retrieval_instruction+query, device=device, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    sim_vector = (embeddings.to(device) @ query_embedding.to(device))\n",
        "    top_10_indices = sim_vector.argsort().cpu().numpy()[::-1][:k]\n",
        "    top_10_chunks = [chunks[i] for i in top_10_indices]\n",
        "    return top_10_chunks"
      ],
      "metadata": {
        "id": "TpgOT4RqF7Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompts\n",
        "base_prompt = \"\"\"\n",
        "# Assistant Task\n",
        "Please answer the user query only with reference to the context passages below.\n",
        "\n",
        "## Context\n",
        "{chunks}\n",
        "\n",
        "## User Query\n",
        "{query}\n",
        "\"\"\".strip()\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a helpful AI assistant who answers questions.\n",
        "\"\"\".strip()\n",
        "\n",
        "# generation functions\n",
        "def init_messages(base_prompt, system_prompt, query, text_chunks):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\":\"user\", \"content\":base_prompt.format(chunks='\\n'.join(text_chunks), query=query)}\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "def generate_response(messages, llm, max_width=70):\n",
        "    current_length = 0\n",
        "\n",
        "    text = ''\n",
        "    for token in llm.create_chat_completion(messages, max_tokens=-1, stream=True):\n",
        "        if 'content' in token['choices'][0]['delta']:\n",
        "            if current_length + len(token['choices'][0]['delta']['content']) + 1 > max_width:\n",
        "                print()\n",
        "                current_length = 0\n",
        "            text += token['choices'][0]['delta']['content']\n",
        "            print(token['choices'][0]['delta']['content'], end='', flush=True)\n",
        "            current_length += len(token['choices'][0]['delta']['content']) + 1\n",
        "    print()\n",
        "    messages.append({\"role\": \"assistant\", \"content\": text})\n",
        "    return messages"
      ],
      "metadata": {
        "id": "bJ0sHRf3iYqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example with functions, getting started\n",
        "chunks = get_text_chunks()\n",
        "embeddings = get_embedding(chunks)\n",
        "\n",
        "query = \"What were the major failures of the Antonines?\"\n",
        "top_10_chunks = semantic_search(query, embeddings, chunks, model)\n",
        "messages = init_messages(base_prompt, system_prompt, query, top_10_chunks)\n",
        "messages = generate_response(messages, llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466,
          "referenced_widgets": [
            "d529f20a1fed48fd932ff525aa27666e",
            "982368cc0afb445e97f3656bf5d82f59",
            "fed18b8518b343088017cece37de8106",
            "8c70555a9d3141a5b04bdd008a562859",
            "67067197449a4d82875ce44d31631bc3",
            "4068b0f14f1642d1805be13ebef55669",
            "40825c5726c44974bf347fb756e1e162",
            "9a190f079c214f7b9e2a662085279a4f",
            "ac4cf65358d84e358cad39313d15bebc",
            "7889e3e425e74209a4f8912c3eb14736",
            "3b62ef3c9dea4fbd8c0042dd0a68c327"
          ]
        },
        "id": "mJxW66wUjvXW",
        "outputId": "047fc332-7304-4727-9044-5fcf37bb11f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d529f20a1fed48fd932ff525aa27666e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The major failures of the Antonines, according to Edward\n",
            " Gibbon's \"History of the Decline and Fall of Rome,\"\n",
            " include:\n",
            "\n",
            "1. Inability to produce original genius or\n",
            " excel in elegant composition: Although they encouraged\n",
            " learning and curiosity, the Antonines did not produce any\n",
            " notable writers of original genius.\n",
            "2. Weakness in\n",
            " administration: The Antonines failed to address the\n",
            " underlying issues that led to the decline of the Roman\n",
            " Empire, such as economic problems, military weakness, and\n",
            " social decay.\n",
            "3. Failure to reform the state: Pertinax\n",
            "'s attempts at reform were cut short by his assassination\n",
            " by the Praetorian Guards.\n",
            "4. Inability to maintain\n",
            " unity: The empire was dismembered under Valerian and\n",
            " Gallienus, reducing it to a low point from which\n",
            " recovery seemed impossible.\n",
            "\n",
            "These failures contributed to\n",
            " the decline of the Roman Empire during the Antonine period\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adding a new message\n",
        "query = \"Tell me about the rise of Christianity in the Empire.\"\n",
        "top_10_chunks = semantic_search(query, embeddings, chunks, model)\n",
        "messages.append({\"role\":\"user\", \"content\":base_prompt.format(chunks='\\n'.join(top_10_chunks), query=query)})\n",
        "messages = generate_response(messages, llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "D30ES460kAIv",
        "outputId": "a05a05b6-1564-4a21-fe5d-9414d4088a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to Edward Gibbon's \"History of the Decline and\n",
            " Fall of Rome,\" the rise of Christianity in the Roman\n",
            " Empire can be attributed to several factors:\n",
            "\n",
            "1. The\n",
            " teachings of Jesus Christ: The message of love, compassion\n",
            ", and forgiveness preached by Jesus Christ resonated with\n",
            " many people, especially those who were marginalized or\n",
            " oppressed by society.\n",
            "2. Persecution: The early\n",
            " Christians faced persecution under various Roman emperors,\n",
            " which only served to strengthen their faith and attract\n",
            " more converts.\n",
            "3. Social conditions: The Roman Empire was\n",
            " experiencing a period of economic decline, political\n",
            " instability, and moral decay, creating an environment in\n",
            " which people were open to new ideas and beliefs.\n",
            "4. The\n",
            " example of the martyrs: The willingness of Christians to\n",
            " die for their faith inspired many others to convert to\n",
            " Christianity.\n",
            "5. The spread of Christianity through trade\n",
            " and commerce: As trade routes expanded, so did the spread\n",
            " of Christianity, as merchants and travelers carried the\n",
            " message of Christ with them.\n",
            "\n",
            "Gibbon notes that while\n",
            " there were other factors at play, such as the causes\n",
            " assigned by Gibbon himself in his fifteenth chapter for\n",
            " the diffusion of Christianity, these are some of the most\n",
            " significant reasons for the rise of Christianity in the\n",
            " Roman Empire."
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-f8fb95457f95>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtop_10_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemantic_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbase_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_10_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-71-703c5b1359ff>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(messages, llm, max_width)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_chat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'content'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_width\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama_chat_format.py\u001b[0m in \u001b[0;36m_convert_text_completion_chunks_to_chat\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mllama_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateCompletionStreamResponse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m ) -> Iterator[llama_types.ChatCompletionChunk]:\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             yield {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m_create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0mfinish_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0mmultibyte_fix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m         for token in self.generate(\n\u001b[0m\u001b[1;32m   1093\u001b[0m             \u001b[0mprompt_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;31m# Eval and sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msample_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 token = self.sample(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             )\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;31m# Save tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_past\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn_past\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/_internals.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         return_code = llama_cpp.llama_decode(\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problems with RAG\n",
        "\n",
        "There are a couple notable limitation with this simple implementation of RAG:\n",
        "\n",
        "\n",
        "1.   Information retrieval is a key part of the process, but if your information retrieval is inaccurate then the RAG response will be likewise inaccurate. We used one technique for information retrieval but there are many more that try to allivate this problem.\n",
        "2.   The prompt is of incredible important to the model. This can feel very arbitrary and difficult to control\n",
        "3.  The LLM can still make mistakes and misunderstand the context of certain text chunks. Better models will do this less, but it is ultimately impossible to completely avoid.\n",
        "\n",
        "\n",
        "That said, you should experiment with the code above and try to resolve these issues in this small example.\n",
        "\n",
        "For any questions, feel free to reach out to peter.nadel@tufts.edu.\n"
      ],
      "metadata": {
        "id": "76oCpzw5liFX"
      }
    }
  ]
}