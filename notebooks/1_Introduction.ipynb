{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers[torch] sentencepiece -Uq\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is classification?\n",
    "\n",
    "The task of a classification model is to classify something as a certain type of that thing. In NLP, there are many applications of this type of model, including sentiment analysis: determining whether a sentence is of positive or negative sentiment or meaning.\n",
    "\n",
    "### Today's example\n",
    "Today, we're going to create our sentiment analyizer using a classification model that will determine whether a text is of positive or negative sentiment. Specifically, this dataset is taken from tweets. Each tweet is labeled as 'Positive', 'Neutral' or 'Negative'. This is a very common way of labeling data, but we will see later in the course that how we label our data can have a profound effect on our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling our data\n",
    "\n",
    "Unfortunately, we are not able to create a model out of raw text. Instead, we have to put into a very specific form so that the software libraries we'll be using can read and process it correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset comes from a website called HuggingFace. This site hosts both pretrained models and datasets to train and finetune new models. We will be using [this dataset](https://huggingface.co/datasets/mteb/tweet_sentiment_extraction) which was collected by the Massive Text Embedding Benchmark. This group uses datasets like this one to evaluate how good certain models are compared to others. What we will be doing in this notebook is a key part of their evlaution process for new models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data comes to us as a HuggingFace Dataset. This is a specific Python-based data structure that is derived from parquet files hosted on the MTEB HuggingFace account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('mteb/tweet_sentiment_extraction')\n",
    "train = dataset['train'].to_pandas()\n",
    "test = dataset['test'].to_pandas()\n",
    "\n",
    "train['label'] = train['label'].astype('float32')\n",
    "test['label'] = test['label'].astype('float32')\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_pandas(train),\n",
    "    'test': datasets.Dataset.from_pandas(test)\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just a fancy spreadsheet\n",
    "dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See! Just a spreadsheet! We'll talk about how I read it into our notebook in a couple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the sentiments in our dataset and how many there are. Each one is associated with a tweet, but they didn't just appear there magically. Someone (or more likely a group of people) had to classify these by hand. As a result, labeled data like this can be extremely valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, we're training our own sentiment analysis model today. We are not starting from nothing. Today we are going to do a process called 'fine-tuning.' Fine-tuning is when we take a model that someone else has trained and we train it again for a specific task. This is very common for classification tasks like this one.\n",
    "\n",
    "Today, we'll be using Microsoft's `deberta` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`deberta` will transform the text of the tweets into mathematical representations of the text that we can use to generate our model.\n",
    "\n",
    "What is important, however, is the the step we take before this conversion from text to numbers: **tokenization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(x):\n",
    "  return tokz(x[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking a sentence into the words that make it up and then associating each piece of the sentence with a number that we can refer to later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = dataset.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this is what a single post looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds['train']['input_ids'][0], tok_ds['train']['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make sure with the `decode` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz.decode(tok_ds['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some way to know how well our model is doing while we train. There are many options to use but today I'll use Pearson's R. This metric will give us updates on our model while its training.\n",
    "\n",
    "As you may have noticed above, we have two sets of data in our larger dataset: a training and test set. This training set is much larger and will be what we finetune our model on. The model will only see the test set after the model has finished looking at hte training set.\n",
    "\n",
    "If the model looks like its going really well, and getting all of the predictions right on the training set, it's possible we have a really good model or a model that has only memorized the answers on the training set, that is a terrible model for using outside of the training set. This is called **overfitting** and is common problem in all machine learning applications.\n",
    "\n",
    "As a result, we use the test set, a set of data that the model has never seen to give use a sense of how well the model is doing beyond the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def corr(x,y):\n",
    "  return np.corrcoef(x,y)[0][1]\n",
    "\n",
    "def corr_d(eval_pred):\n",
    "  return {'pearson': corr(*eval_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer\n",
    "\n",
    "# these three hyperparameters can be changed and tweaked to return a new model.\n",
    "bs = 64\n",
    "epochs = 6\n",
    "lr = 1e-5\n",
    "\n",
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n",
    "trainer = Trainer(model, args, train_dataset=tok_ds['train'], eval_dataset=tok_ds['test'],\n",
    "                  tokenizer=tokz, compute_metrics=corr_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Once training is finished, we can start predicting. These are the predictions for the evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(trainer.eval_dataset)[1]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eval_df = pd.DataFrame({'text':trainer.eval_dataset['text'], 'pred':preds})\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "p = eval_df.sample(5).apply(lambda x: pp.pprint([f\"TEXT: {x['text']}\", f\"PREDICTION: {x['pred']}\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random sample of our evaluation data is looking good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This exercise is the most basic example of using a neural network to read sentiment from a document. There are a lot of optimizations and augmentations that can be implemented to cut down our loss even more.\n",
    "\n",
    "I recommend the following links if you are interested in learning more:\n",
    "* [Practical Deep Learning for Coders](https://course.fast.ai/)\n",
    "* [The NLP HuggingFace Course](https://huggingface.co/course/chapter1/1)\n",
    "* [Andrej Karpathy's Zero to Hero Series](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
