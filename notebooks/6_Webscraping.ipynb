{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Webscraping with Selenium\n",
    "\n",
    "In the last workshop, we saw how to use `BeautifulSoup` to scrape data from a website and read that data into a powerful data structure called a `pandas` `DataFrame`. In this notebook, we'll do something very similar again. We'll be taking a website url, passing it through 3rd party software and extracting useful information that we can use to populate a DataFrame. This time, however, we will be scraping a *dynamic* website, that is a website whose HTML code is generated by an application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Task in this notebook\n",
    "We are going to scrape the headlines from today's issue of the [New York Times](https://www.nytimes.com/). Then, we'll put this data in a `DataFrame` and save it locally as a csv for later use.\n",
    "\n",
    "A note on copyright: all Tufts logins come with the New York Times, so be sure to log into your Tufts account before you continue. Please find instructions on doing so [here](https://researchguides.library.tufts.edu/nytimes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals:\n",
    "* Understand what a dynamic website is and how it is different from a static website\n",
    "* Install `Selenium` along with it's associated dependencies\n",
    "* Navigate the content of the site both in `Selenium` and `BeautifulSoup`\n",
    "* Get more experience with generating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a dynamic website\n",
    "Let's delve a bit deeper into what a dynamic website is and why we can't just use `BeautifulSoup` to parse it as we can with static websites. While a static webpage would require a manual update before content on the site can change, a dynamic website takes advantage of client and server-side scripting to be more adaptable to a user's needs.\n",
    "* Client-side scripting: code that is executed by the user's browser, generally using JavaScript. This scripting renders changes to the site when the user interacts with it. This can be anything from selecting a choice in a drop down menu to full fledged games like Wordle. This type of scripting is also common in many static sites.\n",
    "* Server-side scripting: code that is executed by the server before sending content to the user's browser. This code can be written in a wide varity of languages like Ruby (`RubyOnRails`), JavaScript (`VueJS`, `NodeJS`) and Python (`Django`, `Flask`). This code generally gets inputs from querying a database associated with the site and outputs HTML code from a template. This way, programmers can update elements in their sites without having to rewrite large sections of it. But, it also means that the HTML is not yet generated when we do a get request.\n",
    "\n",
    "Let's look at what this means for us in Python code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what we did in part i of the art of webscraping. if this isn't making any sense, check out [LINK TO PART I]\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "soup = BeautifulSoup(requests.get('https://www.nytimes.com/').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this mess? There is clearly useful information, like headlines, here, but it's really hard to figure out how to scrape it. There doesn't seem to be an inherent structure of the site that we can take advantage of, as we did in the last notebook. Instead, we are going to use `Selenium` to render this site in a browser, so we can then use access the HTML output without dealing with the back-end of the site, which we don't have access to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Selenium`: what is it and how does it works\n",
    "\n",
    "`Selenium` is a software package that automates an instance of a particular browser in the runtime of many programming languages, including Ruby, JavaScript and of course Python.\n",
    "\n",
    "In order to use Selenium in any environment, one must first download the driver to a web browser (we'll be using Chrome) and then also install the software package itself (we'll be using pip).\n",
    "\n",
    "I chose to put this notebook lesson on Colab because the Linux environment makes it very easy to download and install the dependencies needed to use `Selenium`, but if you prefer to use this notebook locally, either run it in a WSL terminal (CHECK ON THIS) or follow the official [documentation](https://selenium-python.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## installing all of the dependencies as well as the selenium package\n",
    "%%shell\n",
    "\n",
    "# Add debian buster\n",
    "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
    "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
    "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
    "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
    "EOF\n",
    "\n",
    "# Add keys\n",
    "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
    "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
    "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
    "\n",
    "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
    "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
    "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
    "\n",
    "# Prefer debian repo for chromium* packages only\n",
    "# Note the double-blank lines between entries\n",
    "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
    "Package: *\n",
    "Pin: release a=eoan\n",
    "Pin-Priority: 500\n",
    "\n",
    "\n",
    "Package: *\n",
    "Pin: origin \"deb.debian.org\"\n",
    "Pin-Priority: 300\n",
    "\n",
    "\n",
    "Package: chromium*\n",
    "Pin: origin \"deb.debian.org\"\n",
    "Pin-Priority: 700\n",
    "EOF\n",
    "\n",
    "# Install chromium and chromium-driver\n",
    "apt-get update\n",
    "apt-get install chromium chromium-driver\n",
    "\n",
    "# Install selenium\n",
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver # the main entry point for the selenium API, the webdriver\n",
    "chrome_options = webdriver.ChromeOptions() # some useful features for the chrome driver\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "wd = webdriver.Chrome(options=chrome_options) # final webdriver object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like when we used `requests`, we can use the get method to load the site into our runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.get('https://www.nytimes.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get the headlines from each article, we need to select the `section` tags. In `BeautifulSoup`, we could use something like `soup.find_all('section')`, but the syntax is slightly different in `Selenium`.\n",
    "\n",
    "Check out the documentation [here](https://selenium-python.readthedocs.io/locating-elements.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.get_attribute('outerHTML') for i in wd.find_elements(By.XPATH, './/section//h3')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, this By class allows us to select many options to search by\n",
    "## here, we'll query by HTML tag\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "sections = wd.find_elements(By.TAG_NAME, 'section') # NOTE: I used find_elements with an 's', if you were to use find_element, it would only return the first element that meets the condiction, which can be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections[6].find_element(By.TAG_NAME,'h3')#.get_attribute('outerHTML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in sections:\n",
    "    if (len(section.text) > 0): # omitting all of the empty titles\n",
    "        print(type(section), section.text.replace('\\n', ''), sep='\\t')\n",
    "    else:\n",
    "        print(type(section), section.find_element(By.TAG_NAME,'h3'), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting there, but there is still a lot of stuff in here that we don't want. We could just remove the end of the list and hard code and indexing statement like `list_of_tags[:idx_of_last_headline]`, but this is dangerous, as the New York Times changes at least everyday, sometimes multiple times a day, so we will use a more robust method of querying the webpage than just HTML tag.\n",
    "\n",
    "Instead, we are going to select headlines by how they are styled on the webpage. Below, I have printed out all of the links of the site, which will have all of the headlines we want. Go through this list and determine which class attributes we want for our headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = wd.find_elements(By.TAG_NAME, 'a')\n",
    "for a in a_tags:\n",
    "  print(a.text, a.get_attribute(\"class\"), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this list, I found `css-9mylee` and `css-rgq5s4` as the two main headline classes. Now we can use XPATH filtering to isolate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here, we are going to use the XPATH attribute of the By class\n",
    "## XPATH is a common practice for XML parsing that can be extended to HTML using XHTML\n",
    "\n",
    "## in order to process two XPATH patterns, we can make a list of them and feed them into\n",
    "## the find_elements method one by one\n",
    "\n",
    "patterns = [\n",
    "    \"//a[@class='css-9mylee']\",\n",
    "    \"//a[@class='css-rgq5s4']\"\n",
    "]\n",
    "\n",
    "for pattern in patterns:\n",
    "  hls = wd.find_elements(By.XPATH, pattern)\n",
    "  for hl in hls:\n",
    "    if (len(hl.text) > 0):\n",
    "      print(hl.text)\n",
    "      print('_____________________') ## print this here so we see where each a tag stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, now there is stuff here, like the author name or how long a read the article is, that isn't the headline and we still haven't been able to scrape the link to the article itself.\n",
    "\n",
    "Now, we want to look at the raw HTML at these place and try to deal with the problems above with BeautifulSoup parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pattern in patterns:\n",
    "  hls = wd.find_elements(By.XPATH, pattern)\n",
    "  for hl in hls:\n",
    "    if (len(hl.text) > 0):\n",
    "      print(hl.get_attribute('outerHTML')) ## this is the only line different from above\n",
    "      print('_____________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we now have the raw HTML, we can call BeautifulSoup and sort out any issues that way.\n",
    "\n",
    "We are going to use the `href` value in the a tag to govern what type of headline we are going to look for in each `a` tag.\n",
    "* First, we don't want any of the games or podcast titles, so we can search the link to see if it has any keywords that would suggest it is one of these. Using the `in` operator and treating the link like any string, we can filter out the results we don't want\n",
    "* Once we have all of the headlines, we can again search the link to see if it is in the opinion section. If it is, then both the author's name and the article's name will be in a `h3` tag, and we can select which one we want accordingly.\n",
    "\n",
    "Note that for both of these operations, I needed a familiarity with the webpage beyond just want it looks like in my browser. I needed to experiement with what worked and follow the HTML of the site as closely as I could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "for pattern in patterns:\n",
    "  hls = wd.find_elements(By.XPATH, pattern)\n",
    "  for hl in hls:\n",
    "    if (len(hl.text) > 0):\n",
    "      soup = BeautifulSoup(hl.get_attribute('outerHTML'))\n",
    "      link = soup.find('a')['href']\n",
    "      if not isinstance(soup.find('h3'), type(None)): ## filtering out any none objects, that is links with no h3 tags\n",
    "        if ('tips' not in link) and ('puzzles' not in link) and ('crossword' not in link) and ('games' not in link) and ('podcasts' not in link) and ('briefing' not in link):\n",
    "          if ('opinion' in link):\n",
    "            headline = soup.find_all('h3')[1].text\n",
    "            print(link, headline)\n",
    "          else:\n",
    "            headline = soup.find('h3').text\n",
    "            print(link, headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now let's move this to a dataframe\n",
    "import pandas as pd\n",
    "headline_dict = {}\n",
    "for pattern in patterns:\n",
    "  hls = wd.find_elements(By.XPATH, pattern)\n",
    "  for hl in hls:\n",
    "    if (len(hl.text) > 0):\n",
    "      soup = BeautifulSoup(hl.get_attribute('outerHTML'))\n",
    "      link = soup.find('a')['href']\n",
    "      if not isinstance(soup.find('h3'), type(None)):\n",
    "        if ('tips' not in link) and ('puzzles' not in link) and ('crossword' not in link) and ('games' not in link) and ('podcasts' not in link) and ('briefing' not in link) and ('theathletic' not in link):\n",
    "          if ('opinion' in link):\n",
    "            headline = soup.find_all('h3')[1].text\n",
    "            headline_dict[headline] = link\n",
    "          else:\n",
    "            headline = soup.find('h3').text\n",
    "            headline_dict[headline] = link\n",
    "\n",
    "headline_df = pd.DataFrame.from_dict(headline_dict, orient='index').reset_index().rename(columns={'index':'headline', 0:'link'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a really clean `DataFrame` that we can use to then get the text from each of these articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use our new Selenium skills to pull the dynamically generated text from the article\n",
    "a_link = headline_df['link'].iloc[9]\n",
    "\n",
    "# here, I am making a new webdriver object, we don't need to but it can be helpful if you have multiple\n",
    "article_wd = webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "article_wd.get(a_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the paragraphs are predictably held in p tags\n",
    "for p in article_wd.find_elements(By.TAG_NAME, 'p'):\n",
    "  print(BeautifulSoup(p.get_attribute('outerHTML')).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there's some text here that we don't want, mostly at the beginning and a couple throoughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in article_wd.find_elements(By.TAG_NAME, 'p'):\n",
    "  print(p.get_attribute('class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the css class `css-at9mc1 evys1bk0` will select all of the text paragraphs, while leaving out the other information we don't want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "for p in article_wd.find_elements(By.TAG_NAME, 'p'):\n",
    "  if p.get_attribute('class') == 'css-at9mc1 evys1bk0':\n",
    "    paragraphs.append(BeautifulSoup(p.get_attribute('outerHTML')).text)\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's formalize this into a function and call it on the whole link column using apply\n",
    "# this cell will take 10-15 minutes to run as the article_wd.get line will take some time to process for each article\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def getParagraphs(link):\n",
    "  paragraphs = []\n",
    "\n",
    "  article_wd = webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "  time.sleep(2)\n",
    "  article_wd.get(link)\n",
    "  for p in article_wd.find_elements(By.TAG_NAME, 'p'):\n",
    "    if p.get_attribute('class') == 'css-at9mc1 evys1bk0':\n",
    "      paragraphs.append(BeautifulSoup(p.get_attribute('outerHTML')).text)\n",
    "  return ' '.join(paragraphs).strip() # this line takes the list of paragraphs and converts them into a single string\n",
    "\n",
    "headline_df['article_text'] = headline_df['link'].progress_apply(getParagraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "Let's see what we can do with this scraped data. This is just a test so that you can see what type of things you can now do with this data in this form. It will be a recap of what it is in the `Gentle Introduction to NLP` workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# sentence-level tokenization\n",
    "headline_df['sents'] = headline_df['article_text'].apply(nltk.sent_tokenize)\n",
    "headline_sents = headline_df.explode('sents').drop(['link','article_text'],axis=1).reset_index(drop=True).dropna()\n",
    "\n",
    "# word-level tokenization\n",
    "headline_sents['words'] = headline_sents['sents'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords are very common words that we don't generally care about in statisical analysis\n",
    "# sometimes, though, you may care about them...\n",
    "# this stops object is a normal Python set, so you can change it accordingly\n",
    "from nltk.corpus import stopwords\n",
    "stops = list(stopwords.words('english'))\n",
    "# i'll add some that are relevent here\n",
    "stops.append('said') # very common in journalism\n",
    "stops.append('would') # these two don't come pre-loaded though they should...\n",
    "stops.append('could')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's add all of the punctuation to the stops list\n",
    "import string\n",
    "for punct in string.punctuation:\n",
    "  stops.append(punct)\n",
    "\n",
    "# nyt uses these special characters\n",
    "stops.append('’')\n",
    "stops.append('“')\n",
    "stops.append('”')\n",
    "stops.append('—')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpliest way to do word frequency... count the words!\n",
    "import collections\n",
    "word_count = collections.defaultdict(int) # creates a dictionary that will increment its value everytime it sees the same key (in this case, every word used multiple times)\n",
    "words = [word.lower() for sent in headline_sents.words.to_list() for word in sent] # all words taken from the DataFrame in a single list\n",
    "\n",
    "for word in words:\n",
    "  if word not in stops:\n",
    "    word_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame.from_dict(dict(sorted(word_count.items(), key=lambda item: item[1], reverse=True)), orient='index').reset_index().rename(columns={'index':'word',0:'count'})\n",
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the top 50 words by frequency\n",
    "count_df[:50].plot.barh(x='word',y='count', figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing what we learned\n",
    "* The difference between dynamic and static websites\n",
    "* How to install `Selenium` in Colab\n",
    "* How to make a `get` request in `Selenium`\n",
    "* Navigating markup (HTML) in `Selenium`\n",
    "* Using `Selenium` and `BeautifulSoup` together to extract text\n",
    "* Pulling text and populating a `DataFrame`\n",
    "* Plotting word frequency, excluding stopwards and punctuation\n",
    "\n",
    "As a challenge, try scraping the New York Times archive. In `Selenium`, though we didn't cover it here, you can input queries into search bars and navigate through dynamically generated results. Read more about it [here](https://selenium-python.readthedocs.io/navigating.html#interacting-with-the-page).\n",
    "\n",
    "Once you have a `DataFrame`, try doing what we did above and calculate word frequency, but not for a given day's articles, but rather for a particular search. You could even package all of it into a single function whose only input is a search string. That way, we could visualize the most important subtopics for any given topic in the history of the Times. If you give that a try and get confused or want to show off what you did, feel free to reach out to me at peter.nadel@tufts.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for reading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
