{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "By Peter Nadel, Digital Humanities Natural Language Processing Specialist\n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "Retrieval Augmented Generation, or RAG, is a technique that can be used to prompt a Large Language Model (LLM) to answer questions from an existing knowledge base. It doesn't not involve any further training of the model, but rather relies on the model's ability to follow instructions.\n",
    "\n",
    "## Two-Step Process\n",
    "Given a user query, RAG decomposes into two main steps. First, we have to go into the knowledge base, what practitioners sometimes call a *corpus*, and retrieve sections of the text that are relevant to the user query. This process is usually known as information retrieval and has been a common task since the inception of NLP. We will implement an algorithm called 'semantic search' to retrieve these text chunks from our corpus. Second, we will pass these retrieved chunks to the LLM in the form of an elaborate prompt, telling the LLM to answer the user query only refering to the information in the context.      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependancies\n",
    "%%capture\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir -q\n",
    "else:\n",
    "  !pip install llama-cpp-python -U\n",
    "\n",
    "!pip install sentence_transformers --no-deps -q\n",
    "!pip install streamlit langchain pypdf python-docx pandas numpy tiktoken huggingface-hub -q\n",
    "!pip install numpy==1.23.5 -q\n",
    "!mkdir BAAI_bge-m3\n",
    "!huggingface-cli download BAAI/bge-m3 --local-dir BAAI_bge-m3 --local-dir-use-symlinks False\n",
    "!chmod -R 755 BAAI_bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependancies\n",
    "import streamlit as st\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pypdf\n",
    "import docx\n",
    "from io import StringIO, BytesIO\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import re\n",
    "import requests\n",
    "import io\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Semantic Search\n",
    "\n",
    "Semantic search utilizes a different language model to represent text chunks in high dimensional vector space, often called an *embedding*. We will split up our corpus into chunks of text so that then we can *embed* it using this model. Once embedded, we can then embed our user query in the same way.\n",
    "\n",
    "We will then have one matrix of shape (number_of_text_chunks, embedding_dimension) and a one vector (1, embedding_dimension). Taking the dot product between this vector and the transpose of this matrix will give us a (1, number_of_text_chunks) vector the values of which represent the **similarity between the query and each text chunk**.\n",
    "\n",
    "In standard semantic search, we would then return the results back to the user, but in the case of RAG, we will use these text chunks as the context in a prompt.\n",
    "\n",
    "**Nota Bene**: The term \"large language model\" was originally coined to describe these embedding models. In fact, they are remarkably similar to their \"generative\" counter parts. The main difference is that an embedding model is trained guess randomly masked words from a larger sequence, whereas what is commonly refered to as an LLM is trained to guess the next word in a large sequence. The former is very good for modeling semantic meaning in texts, and the latter is very good for text completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep\n",
    "\n",
    "In this example, we'll be looking at the first volume of *The Decline and Fall of the Roman Empire* by Edward Gibbon. This is massive text about the the Roman Empire from 200 AD to the Fall of Constninople in 1453. This selection covers the migration and integration of Germanic tribes into the remants of the Western Roman Empire around 400 AD.\n",
    "\n",
    "It is a useful case to explore with RAG as it is very long and thus cannot be given in its totality to an LLM. Instead, we will have to employ RAG so that the LLM answers accurately and quickly.\n",
    "\n",
    "Here we will prepare our data for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the url to the gutenberg project page\n",
    "display(HTML(\n",
    "    \"\"\"<iframe src=\"https://gutenberg.org/cache/epub/731/pg731.txt\"></iframe>\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://gutenberg.org/cache/epub/731/pg731.txt') # using requests to get the text\n",
    "text = res.text\n",
    "text[10000:10500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Now that we have our text, we need to embed it. Before we can do so, we need to split it up in to chunks that can be read by the embedding model. This process, known as chunking, can have a profound effect on the output of our RAG. If our chunks are too small or too big then our context will be useless, so it often comes down to experiementation.\n",
    "\n",
    "We will use an off-the-shelf chunker from `langchain` for this example, but I encourage your to design your own. I've given a default `chunk_size` of 250 tokens and `chunk_overlap`, how much from the last chunk should be carried over into the current chunk, of 50 tokens. Play around with this and see the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=250, chunk_overlap=50) # langchain tokentextsplitter, other exists: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\n",
    "chunks = text_splitter.split_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model: `bge-m3`\n",
    "\n",
    "Below we begin the embedding process for our corpus. We are using this embedding model: [bge-m3](https://huggingface.co/BAAI/bge-m3). I like this one from experimentation but there are many other that you can find on HuggingFace. I encourage you to experiment with this choice as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # move data to the gpu\n",
    "model = SentenceTransformer('/content/BAAI_bge-m3')\n",
    "\n",
    "embeddings = model.encode(\n",
    "    chunks, # text input\n",
    "    batch_size=64, # batch size\n",
    "    device=device, # gpu device\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True, # converts to Pytorch tensor\n",
    "    normalize_embeddings=True # allows us to compare embeddings\n",
    ")\n",
    "\n",
    "embeddings.shape # (number_of_text_chunks, embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information retrieval\n",
    "\n",
    "Now we are ready to start the information retrieval process. As mentioned above, we will take a question from the user, here `query` and search for similar text chunks with a matrix multiplication. We will then extract the indices of the most relevant chunks and compile them into a form to be read by our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_instruction = \"Represent this sentence for searching relevant passages: \" # need to prepend this string\n",
    "query = \"What were the major goals of the Antonines?\" # user query\n",
    "query_embedding = model.encode(retrieval_instruction+query, device=device, convert_to_tensor=True, normalize_embeddings=True)\n",
    "sim_vector = (embeddings.to(device) @ query_embedding.to(device)) # matmul to compare query_embedding to embeddings\n",
    "sim_vector.shape # simiarlity scores between each embedding and the query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_vector.argsort() # sorted by index, same index as chunk_list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_vector.argsort().cpu().numpy() # takes off of gpu and converts to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_vector.argsort().cpu().numpy()[::-1] # reverses list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_indices = sim_vector.argsort().cpu().numpy()[::-1][:10] # get top 10, this number is arbitrary\n",
    "top_10_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_chunks = [chunks[i] for i in top_10_indices]\n",
    "top_10_chunks # most relevant chunks to our search query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generation\n",
    "\n",
    "With our relevant text chunks, we can now move on to generating an answer to our user query. Prompt engineering can be deceptively difficult. I've provided a very simple prompt below, but feel free to change it and see how that affects the output of the model.\n",
    "\n",
    "For this example, we are using `llama-cpp-python` to load the pre-quantized version of [this LLM](https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B). This is finetuned version of the base Llama-3-8B model and is particularly good a following user instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what can we do with these chunks?\n",
    "base_prompt = \"\"\"\n",
    "# Assistant Task\n",
    "Please answer the user query only with reference to the context passages below.\n",
    "\n",
    "## Context\n",
    "{chunks}\n",
    "\n",
    "## User Query\n",
    "{query}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_prompt = base_prompt.format(chunks='\\n'.join(top_10_chunks), query=query) # filling our prompt with our text chunks\n",
    "filled_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model onto the GPU\n",
    "llm = Llama.from_pretrained(\n",
    "        repo_id=\"NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF\",\n",
    "        filename=\"*Q4_K_M.gguf\",\n",
    "        verbose=False,\n",
    "        n_gpu=-1,\n",
    "        n_ctx=5000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant who answers questions.\"}, # system level prompt, feel free to experiement with this too\n",
    "    {\"role\":\"user\", \"content\":filled_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamed response\n",
    "max_width = 70\n",
    "current_length = 0\n",
    "\n",
    "text = ''\n",
    "for token in llm.create_chat_completion(messages, max_tokens=-1, stream=True):\n",
    "    if 'content' in token['choices'][0]['delta']:\n",
    "        if current_length + len(token['choices'][0]['delta']['content']) + 1 > max_width:\n",
    "            print()\n",
    "            current_length = 0\n",
    "        text += token['choices'][0]['delta']['content']\n",
    "        print(token['choices'][0]['delta']['content'], end='', flush=True)\n",
    "        current_length += len(token['choices'][0]['delta']['content']) + 1\n",
    "print()\n",
    "messages.append({\"role\": \"assistant\", \"content\": text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full process\n",
    "\n",
    "Below I will refactor the code above into a couple functions for your ease of use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sem search functions\n",
    "def get_text_chunks(chunk_size=250, chunk_overlap=50):\n",
    "    res = requests.get('https://gutenberg.org/cache/epub/731/pg731.txt')\n",
    "    text = res.text\n",
    "\n",
    "    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def get_embedding(chunks):\n",
    "    model = SentenceTransformer('/content/BAAI_bge-m3')\n",
    "    embeddings = model.encode(\n",
    "        chunks,\n",
    "        batch_size=64,\n",
    "        device='cuda',\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "def semantic_search(query, embeddings, chunks, model, device='cuda', k=10):\n",
    "    retrieval_instruction = \"Represent this sentence for searching relevant passages: \" # need to prepend this string\n",
    "    query_embedding = model.encode(retrieval_instruction+query, device=device, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    sim_vector = (embeddings.to(device) @ query_embedding.to(device))\n",
    "    top_10_indices = sim_vector.argsort().cpu().numpy()[::-1][:k]\n",
    "    top_10_chunks = [chunks[i] for i in top_10_indices]\n",
    "    return top_10_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "base_prompt = \"\"\"\n",
    "# Assistant Task\n",
    "Please answer the user query only with reference to the context passages below.\n",
    "\n",
    "## Context\n",
    "{chunks}\n",
    "\n",
    "## User Query\n",
    "{query}\n",
    "\"\"\".strip()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful AI assistant who answers questions.\n",
    "\"\"\".strip()\n",
    "\n",
    "# generation functions\n",
    "def init_messages(base_prompt, system_prompt, query, text_chunks):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\":\"user\", \"content\":base_prompt.format(chunks='\\n'.join(text_chunks), query=query)}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def generate_response(messages, llm, max_width=70):\n",
    "    current_length = 0\n",
    "\n",
    "    text = ''\n",
    "    for token in llm.create_chat_completion(messages, max_tokens=-1, stream=True):\n",
    "        if 'content' in token['choices'][0]['delta']:\n",
    "            if current_length + len(token['choices'][0]['delta']['content']) + 1 > max_width:\n",
    "                print()\n",
    "                current_length = 0\n",
    "            text += token['choices'][0]['delta']['content']\n",
    "            print(token['choices'][0]['delta']['content'], end='', flush=True)\n",
    "            current_length += len(token['choices'][0]['delta']['content']) + 1\n",
    "    print()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": text})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with functions, getting started\n",
    "chunks = get_text_chunks()\n",
    "embeddings = get_embedding(chunks)\n",
    "\n",
    "query = \"What were the major failures of the Antonines?\"\n",
    "top_10_chunks = semantic_search(query, embeddings, chunks, model)\n",
    "messages = init_messages(base_prompt, system_prompt, query, top_10_chunks)\n",
    "messages = generate_response(messages, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a new message\n",
    "query = \"Tell me about the rise of Christianity in the Empire.\"\n",
    "top_10_chunks = semantic_search(query, embeddings, chunks, model)\n",
    "messages.append({\"role\":\"user\", \"content\":base_prompt.format(chunks='\\n'.join(top_10_chunks), query=query)})\n",
    "messages = generate_response(messages, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with RAG\n",
    "\n",
    "There are a couple notable limitation with this simple implementation of RAG:\n",
    "\n",
    "\n",
    "1.   Information retrieval is a key part of the process, but if your information retrieval is inaccurate then the RAG response will be likewise inaccurate. We used one technique for information retrieval but there are many more that try to allivate this problem.\n",
    "2.   The prompt is of incredible important to the model. This can feel very arbitrary and difficult to control\n",
    "3.  The LLM can still make mistakes and misunderstand the context of certain text chunks. Better models will do this less, but it is ultimately impossible to completely avoid.\n",
    "\n",
    "\n",
    "That said, you should experiment with the code above and try to resolve these issues in this small example.\n",
    "\n",
    "For any questions, feel free to reach out to peter.nadel@tufts.edu.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
