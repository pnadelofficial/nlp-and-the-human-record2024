{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pretrained models for Named Entity Recognition (NER)\n",
    "\n",
    "In this notebook, we are going to explore an important subfield of natural language processing, named entity recognition or NER.\n",
    "\n",
    "By the end of today's class you'll be able to:\n",
    "* Use a pretrained `spaCy` model to find named entities, especially for a non-English language\n",
    "* Explain why finding named entities is challenging without the use of a pretrained token classification model\n",
    "* Employ list comprehensions and advanced dictionaries in Python to parse model output\n",
    "* Install spaCy and download associated models in a Colab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NER and why does it matter?\n",
    "\n",
    "Named entity recognition describes any method which uses computational methods to extract from unstructured text names of people, places or things. It is a hard classification task, meaning that every word in a document is either a type of named entity or it is not. For example in the following sentences:\n",
    "> My name is Peter Nadel. I work at Tufts University.\n",
    "\n",
    "the token 'Peter Nadel' could be tagged as a PERSON tag, where as Tufts Univerisity could be tagged with a PLACE tag. Importantly, in NER, no token can receive more than one tag.\n",
    "\n",
    "As a result, NER can be using in a wide variety of fields and applications.\n",
    "\n",
    "## How do you do NER?\n",
    "Just like many other NLP tasks, there are two main ways of conducting NER:\n",
    "1. **Rules-based**: This approach involves developing a list of rules which can identify a named entity deterministically. For example, if we wanted to identify someone's name, we would develop a rule like: find two words that are capitalized next to each other. This has the advantage that we will always find the entities we have rules for, but as the disadvantage that we have to make a huge amount of rules for this approach to be effective.  \n",
    "2. **Machine learning**: This apporach involves collecting and manually annotating many examples of what named entities look like in context. We can then teach a computer what a named entity looks like, allowing it to identify named entities in new texts. This has the advantage that we don't need to know exactly what a named entity looks like to work, but requires considerable manual annotation to get started.\n",
    "\n",
    "In this notebook, we will use a machine learning *model* to conduct NER. This will be a *pretrained* model, meaning that someone else already spent the time and energy to make it so that it works and we don't need to worry about that. (However, later in the course we will train an NER model from scratch.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for NER\n",
    "\n",
    "We'll be using a package called `spaCy` to conduct our NER. `spaCy` has a variety of pretrained models that we can take advantage of. The number of languages that `spaCy` support is somewhat small, but through this class we'll see how we can supplement it with other languages. For this example, we'll use `LatinCy`, a `spaCy` module for the Latin language. The model we'll be using was trained by [Patrick Burns](https://isaw.nyu.edu/people/staff/patrick-burns), a researcher at NYU's Institute for the Study of the Ancient World.\n",
    "\n",
    "Both `spaCy` and `LatinCy` do not come with this Colab notebook by default, so we'll need to install them. We will be using `pip`, a command line tool for installing Python packages, to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations: recall that we use the '!' to indicate that this is a shell command\n",
    "# this cell will take about 5 min to run\n",
    "!pip install spacy transformers\n",
    "!python -m spacy download en_core_web_lg\n",
    "!pip install \"la-core-web-lg @ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `spaCy` for Named Entity Recognition\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English examples\n",
    "\n",
    "Before we turn to `LatinCy`, let's take a look at what this task looks like for some simple English texts. Then we can apply the same rationale to using the Latin model wiht complex Latin texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "english_nlp = spacy.load('en_core_web_lg') # nlp object takes in the model name and give us back a tool we can work with\n",
    "english_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from above\n",
    "text = \"\"\"\n",
    "My name is Peter Nadel. I work at Tufts University.\n",
    "\"\"\".strip()\n",
    "doc = english_nlp(text) # call english_nlp with text to get a doc object\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entities\n",
    "entities = doc.ents\n",
    "for i, entity in enumerate(entities):\n",
    "  print(f\"Entity {i+1}: \", entity.text, \"| Entity Type: \", entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try a more complex example: the opening of middlemarch by goerge eliot\n",
    "text = \"\"\"\n",
    "Miss Brooke had that kind of beauty which seems to be thrown into relief by poor dress.\n",
    "Her hand and wrist were so finely formed that she could wear sleeves not less bare of style\n",
    "than those in which the Blessed Virgin appeared to Italian painters; and her profile as\n",
    "well as her stature and bearing seemed to gain the more dignity from her plain garments,\n",
    "which by the side of provincial fashion gave her the impressiveness of a fine quotation from\n",
    "the Bible,—or from one of our elder poets,—in a paragraph of to-day’s newspaper.\n",
    "She was usually spoken of as being remarkably clever, but with the addition that\n",
    "her sister Celia had more common-sense. Nevertheless, Celia wore scarcely more\n",
    "trimmings; and it was only to close observers that her dress differed from her sister’s,\n",
    "and had a shade of coquetry in its arrangements; for Miss Brooke’s plain dressing was\n",
    "due to mixed conditions, in most of which her sister shared. The pride of being ladies\n",
    "had something to do with it: the Brooke connections, though not exactly aristocratic,\n",
    "were unquestionably “good:” if you inquired backward for a generation or two, you\n",
    "would not find any yard-measuring or parcel-tying forefathers—anything lower than an\n",
    "admiral or a clergyman; and there was even an ancestor discernible as a Puritan gentleman\n",
    "who served under Cromwell, but afterwards conformed, and managed to come out of all\n",
    "political troubles as the proprietor of a respectable family estate. Young women of\n",
    "such birth, living in a quiet country-house, and attending a village church hardly\n",
    "larger than a parlor, naturally regarded frippery as the ambition of a huckster’s daughter.\n",
    "Then there was well-bred economy, which in those days made show in dress the first item\n",
    "to be deducted from, when any margin was required for expenses more distinctive of rank.\n",
    "Such reasons would have been enough to account for plain dress, quite apart from religious\n",
    "feeling; but in Miss Brooke’s case, religion alone would have determined it; and Celia\n",
    "mildly acquiesced in all her sister’s sentiments, only infusing them with that common-sense\n",
    "which is able to accept momentous doctrines without any eccentric agitation. Dorothea\n",
    "knew many passages of Pascal’s Pensees and of Jeremy Taylor by heart; and to her the destinies\n",
    "of mankind, seen by the light of Christianity, made the solicitudes of feminine fashion\n",
    "appear an occupation for Bedlam. She could not reconcile the anxieties of a spiritual\n",
    "life involving eternal consequences, with a keen interest in gimp and artificial protrusions\n",
    "of drapery. Her mind was theoretic, and yearned by its nature after some lofty conception of\n",
    "the world which might frankly include the parish of Tipton and her own rule of conduct\n",
    "there; she was enamoured of intensity and greatness, and rash in embracing whatever\n",
    "seemed to her to have those aspects; likely to seek martyrdom, to make retractations,\n",
    "and then to incur martyrdom after all in a quarter where she had not sought it. Certainly\n",
    "such elements in the character of a marriageable girl tended to interfere with her lot,\n",
    "and hinder it from being decided according to custom, by good looks, vanity, and merely\n",
    "canine affection. With all this, she, the elder of the sisters, was not yet twenty, and\n",
    "they had both been educated, since they were about twelve years old and had lost their\n",
    "parents, on plans at once narrow and promiscuous, first in an English family and afterwards\n",
    "in a Swiss family at Lausanne, their bachelor uncle and guardian trying in this way to\n",
    "remedy the disadvantages of their orphaned condition.\n",
    "\"\"\".strip().replace(' \\n', ' ')\n",
    "doc = english_nlp(text)\n",
    "entities = doc.ents\n",
    "for i, entity in enumerate(entities):\n",
    "  print(f\"Entity {i+1}: \", entity.text, \"| Entity Type: \", entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot more entities, so let's start store this data in a data structure. In Introduction to Digital Humanities, you probably saw how to count words in a block of text. Here we'll do a similar thing but first we'll count the number of times an entity is mentioned and then we'll count how many times a entity type is mentioned.\n",
    "\n",
    "And we'll actually do both of these in two different ways:\n",
    "* `defaultdict`: a default dictionary is a data structure in Python that functions like a dictionary, but the values are of a certain type.\n",
    "* `Counter`: a dictionary that is designed for counting discrete elements of an list or string.\n",
    "\n",
    "Additionally, for the `Counter`, we'll need to separate the entities list out into a list of entities and a list of their labels. To do so, we'll use list comprehensions. A list comprehension is a special Python syntax that allows us to put a loop on a single line. See the example below:\n",
    "\n",
    "```python\n",
    "# normal for loop\n",
    "holder = []\n",
    "for element in elements:\n",
    "    holder.append(element)\n",
    "```\n",
    "``` python\n",
    "# list comprehension\n",
    "holder = [element for element in elements]\n",
    "```\n",
    "Importantly, these two blocks of code do the same thing, it's just that the list comprehension is on a single line. This can help with efficiency (though only for small- to medium-sized lists) and is easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method one: defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "entity_counts = defaultdict(int)\n",
    "entity_type_counts = defaultdict(int)\n",
    "\n",
    "# for loop for incrementing\n",
    "for entity in entities:\n",
    "  entity_counts[entity.text] += 1\n",
    "  entity_type_counts[entity.label_] += 1\n",
    "\n",
    "# top 3 of each\n",
    "for entity_type, count in sorted(entity_type_counts.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "  print(f\"{entity_type}: {count}\")\n",
    "print('-'*10)\n",
    "for entity, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "  print(f\"{entity}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method two: Counter\n",
    "from collections import Counter\n",
    "\n",
    "# we need a two lists for entities and labels\n",
    "entity_texts = [ent.text for ent in entities]\n",
    "entity_labels = [ent.label_ for ent in entities]\n",
    "\n",
    "entity_counts = Counter(entity_texts)\n",
    "entity_type_counts = Counter(entity_labels)\n",
    "\n",
    "# top 3 of each\n",
    "for entity_type, count in entity_type_counts.most_common(3):\n",
    "  print(f\"{entity_type}: {count}\")\n",
    "print('-'*10)\n",
    "for entity, count in entity_counts.most_common(3):\n",
    "  print(f\"{entity}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now even plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(list(entity_counts.keys()), list(entity_counts.values()))\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Entity')\n",
    "plt.title('Entity Counts')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(list(entity_type_counts.keys()), list(entity_type_counts.values()))\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Entity Type')\n",
    "plt.title('Entity Type Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-English case: Parsing Latin texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('la_core_web_lg') # loading the latin model instead of the english one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection and scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this example we'll use Cicero's Letter's to Atticus\n",
    "# here we download it in XML form and parse it with BeautifulSoup4\n",
    "# if you don't remember this from the intro class, don't worry we'll revisit this in week 5\n",
    "!wget https://www.perseus.tufts.edu/hopper/dltext?doc=Perseus%3Atext%3A1999.02.0008 -O atticus.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(open('atticus.xml', 'r').read(), features='xml')\n",
    "soup.find('div2') # first letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # need to use regular expressions to do some cleaning, we'll revisit this too\n",
    "\n",
    "letters = []\n",
    "for d in soup.find_all('div2'):\n",
    "    dateline = d.dateline.extract().get_text().strip()\n",
    "    salute = d.salute.extract().get_text().strip()\n",
    "    text = re.sub(r'\\s+', ' ', d.get_text().strip().replace('\\n', ''))\n",
    "    letters.append([dateline, salute, text])\n",
    "\n",
    "print(letters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can use pandas to store the data for each letter\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(letters, columns=['dateline', 'salute', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example parse with one letter\n",
    "first_letter = df.text.iloc[0]\n",
    "first_letter_doc = nlp(first_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_letter_entities = first_letter_doc.ents\n",
    "for i, entity in enumerate(first_letter_entities):\n",
    "    print(f\"Entity {i+1}: \", entity.text, \"| Entity Type: \", entity.label_, \"| Entity Lemma: \", entity.lemma_)\n",
    "# here I also print out the words lemma, the base form of the word for counting purposes\n",
    "# more on this next week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_counts(text):\n",
    "    doc = nlp(text)\n",
    "    entities = doc.ents\n",
    "    entity_texts = [ent.lemma_ for ent in entities] # counting lemmas not text\n",
    "    entity_labels = [ent.label_ for ent in entities]\n",
    "    entity_counts = Counter(entity_texts)\n",
    "    entity_type_counts = Counter(entity_labels)\n",
    "    return entity_counts, entity_type_counts\n",
    "\n",
    "df['entity_counts'] = df.text.apply(get_entity_counts)\n",
    "df['entity_type_counts'] = df.entity_counts.apply(lambda x: x[1]) # taking the type counts\n",
    "df['entity_counts'] = df.entity_counts.apply(lambda x: x[0]) # taking the lemma counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entity_counts = df.entity_counts.sum()\n",
    "all_type_counts = df.entity_type_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting the plot below to 15 so that there aren't too many\n",
    "top_15_entities = sorted(all_entity_counts.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "top_15_entities = dict(top_15_entities)\n",
    "top_15_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(list(top_15_entities.keys()), list(top_15_entities.values()))\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Entity')\n",
    "plt.title('Entity Counts')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(list(all_type_counts.keys()), list(all_type_counts.values()))\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Entity Type')\n",
    "plt.title('Entity Type Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've seen today how using specialized, pretrained models can help us do tasks like named entity recongition. We also worked on our Python skills in data parsing and plotting. In the next class, we will discuss some of the other features of `spaCy` models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
